---
title: "Outline for Causal Bayes Paper"
output:
  knit: function() stop("NO KNIT")
---


Outline

- **Intro**
  - *Hook?*
    - value of experiments discussed as degree of updating
    - Bayesian updating often uses experimental research as toy examples
  - outline
  - Assumptions about reader knowledge?
  - *Introduction to both realms*
- **Goals and Tactics**
  - *Shared goals vs different tactics*
    - Best parameter estimates
      - what is best? Unconfounded by design and plausible in estimation
    - how each does it
      - "Identification by design" (Samii 2016)
        - identification analysis: typically conditional independence (identifying variation) and positivity (overlap)
        - internal validity and "pseudo-facts" (identifying variation)
        - external validity, "pseudo-generalization" (common support/positivity/model dependence)
        - Regression based analysis suffers from little overlap in the effective sample (positivity), irresponsible use or interpretation of covariates; the covariates included and the interpretation of the effect depend greatly on the assumed causal graph among the covariates (post-treatment, whatever Nyhan, ABS, Samii, Keele DAG)
        - PILLARS
          - identification by design: how do we get identifying variation
          - specificity of causal facts: what is the variation of the TE being returned?
      - the posterior distribution
        - improvement with priors
        - Bayes is not a tool for "observational" analysis, nor are priors able to address confounding in any coherent, general way (Rubin 1991)
        - we make assumptions because they get us somewhere valuable?
      - biasing and de-biasing
        - designs ≠ estimators
        - good data better than good model (Gelman), but good model makes good data better
        - sometimes an easy nonparametric estimator is implied by identification analysis, and other times not as easy (Keele PA?)
        - All problems have some natural constraints on what the parameter values could be, and using this information can make a difference
    - We do want to make posterior inferences
      - you really do want $p(\theta \mid y)$
    - regularization
    - choice of family using entropy/information principles
    - Where do we see overlap already?
      - examples
    - Logit example?
      - do we cover the parameter more with like a normal(0, 4) prior than with a flat prior?
  - *Potential outcomes with Bayesian Language*
    - `McElreath "without frequentist language" lecture?`
- **Epistemic Norms and Modeling Assumptions**
  - *Within Causal Inference*
    - Skepticism of modeling assumptions
      - confounding
      - vs. estimator bias
      - identification model ≠ statistical model
    - Simplicity of methods
      - Difference in means, linearity
      - but not always. mediation, matching, synthetic control all have model components
      - Repeated sampling properties of estimators depend on outcome distribution so linearity isn't always best
    - Modesty of claims
      - diff in means is one thing
      - Secondary findings are often very model driven 
      - e.g. mediation analyses, analyses with controls
      - "Pattern Specificity" (keele, Rosenbaum, Cook/Shadish) as actually a set of priors
    - Researcher degrees of freedom
      - Mean-zero priors are always going to be more conservative than flat priors
      - holding the functional form fixed, information imposes more skepticism of the data
    - fixed v random effects
  - *Clarifications about Bayes*
    - BCI is benign and banal
    - against "subjective"
    - "Subjectivist" objections are really responding to a regime of priors that probably we wouldn't want to be using for these circumstances
      - Kadane 2008: "As human beings, we are not endowed with the ability to identify objective truth. Thus to treat priors and likelihoods as subjective statements of belief is merely to admit what is manifestly the case."
  - *More general retorts*
    - unbiasedness
  - *Bayes*
    - Assumptions
      - state them for the record
      - entropy
      - but? admit that it doesn't get you out of confounding
      - doesn't have to be *more* assumptions, just making them Bayesian and explicit
    - Conservatism in a different form
      - modeling all sources of variance
      - regularization
      - "focused skepticism" of getting yanked by "nonsense" parameters
    - Designs are downstream from priors?
      - the decision to do $a$ or $b$ enter because the researcher has priors about what would happen if not doing those things. Sometimes those decisions can be incorporated into the model and given a prior
    - What Bayes isn't doing
      - fixing confounding
      - using priors to stack in favor of hypotheses
- **Practical Bayes for Skeptical Causal Inference**
  - *Misunderstandings*
    - Beliefs
    - Researcher degrees of freedom
      - Is something "wrong" because Bayes is used?
        - For ppl who do Bayes, it isn't like frequentism is default and Bayes is something that you do when something goes wrong
        - it's just a different framework for statistical inference
        - It affects what we can and can't say about the results, and we happen to care about this
      - Is there more potential for differing results?
        - One the one hand, potentially, but that's research
        - The point is to scrutinize the model's exposed assumptions
        - sometimes the results *will* look differently, sometimes they will look similar, it depends on the problem
  - *Focus on model-heavy designs*
    - "we don't need priors if we have the design"
    - You won't replace the DIMs, so don't try
  - *Controlling assumptions with WIPs and PPCs*
    - WIPs
      - Natural constraints
      - Parameterization and explosive priors
      - Flat priors even when on the right scale
      - Examples:
        - Vote shares
        - Logistic regression
        - Variances
      - Maximum entropy
    - PPCs
    - practice safe GLMs
  - *Hierarchical modeling*
    - multiple sources of variation
      - repeated obs
      - clusters
      - blocks
      - other levels of heterogeneity
    - partial pooling
      - combining information so as not to throw away information
    - regularization
      - overfitting: worse in-sample fits, better out-sample fits
      - WAIC
      - fundamentally conservative
  - *questions I still have*
    - reference priors/default priors/jeffreys priors
    - objective/subjective bayes
  - *Sensitivity testing for ambiguous estimates using priors*
    - You form different conclusions from different starting points
  - *Direct posterior inference*
    - there's no substitute
    - type-M and -S errors framework given crises in bias and replication
  - *Bayes factors*
    - the "updating factor" between prior odds and posterior odds
  - *When to do it*
    - model-heavy designs
    - cluster-randomization
    - repeated observations
    - many treatments (multiple comparisons)
    - secondary analyses (which tend to rely more on models and controls)
    - stabilizing things when you're slicing and dicing in some robustness check
- **Doing CBE**
  - *RDD*
    - we happen to get similar-ish effects
    - but we ruled out impossibilities
    - So we feel better because we actually did the work
  - *Conjoint*
  - *List experiment?*
- **Difficult or unresolved issues**
  - Pre-analysis and prior checking
  - Non-Bayesian methods with similar benefits
    - Cross-validation and regularized estimators
  - What about randomization inference?
- **Conclusion**


Read and incorporate

- elicited priors
- high stakes experiments we want to get this right



- Incremental research via experimentation and other causal inference approaches (the promise of "empirical Bayes"?)

Social Science Citation Index 
TESS search

  - Subjective priors & unhelpful postmodernism

    - beliefs vs "information"


Hesitations

- let data speak
- conservatism
- nonparametric
  - Mostly Harmless (Business school) for a defense of parametric modeling
  - start with the nonparametric model and build assumptions from there? Or like, what evidence do you have that the nonparametric model is a bad idea or is infeasible?
  - You can always assume that the estimate is a draw from the true effect size, so you can compute the nonparametric estimate and then weight it against the prior. This is what the "eight schools" example does and what I'm doing in the Reeves et al. example
  - Even "semi-parametric" designs contain a number of threshold decisions that could be made fuzzy with reasonable priors
    - RD bandwidth (spike prior)
    - Covariate matching calipers (spike prior)
    - propensity score matching (propensity)
- power analysis and expectations of coverage
- minimal assumptions
- simplified assumptions


answer: 

  - Power analysis
- "thin" models with weakly informative priors
    - Inference paradigm
    - Computational convenience


   
   - Desire
  - pollinating of strong minds and strong methods 



What's the point of including estimates in your "posterior" that make no sense? 


# Complexity

Not all experiments or causal inference designs are simple

Conjoint


# positive case

<https://statmodeling.stat.columbia.edu/2015/12/05/28262/> 

- What I’m saying is that, whatever causal inference framework is being used, I think when extrapolating it is appropriate to use hierarchical models to partially pool. I don’t think of hierarchical models as a competitor or alternative to your causal inference methods; I see hierarchical modeling as an approach that can be used under any framework, whether it be yours or Rubin’s or some other causal framework used in epidemiology, or whatever.


