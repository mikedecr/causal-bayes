---
# global document parameters
title: |
  Bayesian Causal Inference in Political Science^[
    Prepared for the Minnesota Political Methodology Colloquium Graduate Student Conference, April 2019.
  ]
author: |
  Michael G. DeCrescenzo^[Ph.D. Candidate, Political Science, University of Wisconsin--Madison. Email: [`decrescenzo@wisc.edu`](mailto:decrescenzo@wisc.edu). I have received plenty of great feedback and advice from Matthew Blackwell, Barry Burden, William Christiansen, Andrew Heiss, Devin Judge-Lord, Michael Masterson, Anna Meier, Laura Meinzen-Dick, Evan Morier, Ellie Powell, Daniel Putman, Alex Tahk, Zach Warner, Jessica Weeks, and Dave Weimer, and Chagai Weiss.]
# soon: Renshon, Shalef
# send to: Blackwell, Jason A, Ryan P, Warner, Ben, Chagai, Heiss

date: |
  April 12, 2019
# This version: `r format(Sys.time(), '%B %d, %Y')`.^[ View most recent committed version [here](https://github.com/mikedecr/causal-bayes/blob/master/writing/causal-bayes-paper.pdf).]
abstract: |
  Causal inference and Bayesian analysis are two powerful and attractive methodological approaches for conducting empirical research, but rarely in political science does a single study employ both approaches. This stands in contrast to other fields---such as psychology, epidemiology, and biostatistics---where Bayesian and causal methods are more commonly applied together. In this paper I argue that the partition between these methodological schools in political science has no inherent basis in their fundamental goals, which are actually quite compatible: generating the best parameter estimates. In fact, Bayesian analysis provides a number of distinct benefits for estimating statistical models for causal inference. The methodological partition instead owes itself to informal norms surrounding each school in empirical political science. I discuss these sources of normative tension, describe go-to practices doing Bayesian inference for a skeptical audience, and demonstrate these practices using real examples from recent political science publications. The purpose of the paper is *not* to convince all causal inference practitioners to adopt Bayesian estimation. The purpose is to show that Bayesian methods deserve space in the study of causal effects because they improve causal estimates and provide an appealing framework for evaluating causal evidence.

  \begin{center}
    \textbf{\sffamily 
      \faSmileO\ This paper is a work in progress \faSmileO \\ 
      \faSmileO\ Please use care if citing or circulating \faSmileO\ 
    }
  \end{center}

# \begin{flushleft} 
#   \textbf{Keywords}: experiments, causal inference, Bayesian statistics 
# \end{flushleft}

# Declarations: siloing should stop? norms should be changed?
# Be more specific about Bayesian advantages?
# - multiple comparisons
# - sources of variation
# - natural constraints
# Bayesian tactics?
# - WIPS
# - Regularization
# - sensitivity tests for weak evidence/the meaning of "null" findings


bibliography: "/Users/michaeldecrescenzo/Dropbox/bib.bib"
biblio-style: "/Users/michaeldecrescenzo/Dropbox/apsa-leeper.bst"
fontsize: 12pt
geometry: margin = 1.25in
# geometry: margin = 1in
indent: true
linkcolor: black
urlcolor: violet
citecolor: black
subparagraph: yes
classoption: 
  - final
output:
  bookdown::pdf_document2: 
    latex_engine: pdflatex
    toc: false
    toc_depth: 1
    keep_tex: true
    includes: 
      in_header: 
        - assets/rmd-preamble.tex
    number_sections: true # true?
    highlight: kate
    fig_caption: true
    citation_package: natbib
---


<!-- First page parameters -->
\pagenumbering{roman}
\newpage

<!-- TOC page -->
<!-- \tableofcontents
\newpage -->

<!-- BODY -->
\hypersetup{pageanchor = true}
\pagenumbering{arabic}
<!-- \twocolumn -->
\onehalfspacing
<!-- \doublespacing -->
<!--  -->


```{r, eval = FALSE, echo = FALSE, include = FALSE}
rmarkdown::render(here::here("writing", "causal-bayes-paper.Rmd"))
```

```{r packages, include = FALSE, cache = FALSE}
# packages
library("knitr")
library("here")
library("magrittr")
library("tidyverse")
library("ggplot2")
library("patchwork")
library("scales")
library("labelled")
library("broom")
library("latex2exp")
library("tidybayes")

# Knitr chunks:
knitr::opts_chunk$set(
  eval = TRUE, echo = FALSE, include = FALSE, 
  warning = FALSE, message = FALSE,
  cache = TRUE, collapse = TRUE,
  fig.path = here::here("writing", "render-figs/"),
  cache.path = here::here("writing", "render-cache/"),
  dev = "cairo_pdf", fig.align = "center"
)

# graphics theme
theme_set(
  ggthemes::theme_base(base_family = "Fira Sans", base_size = 13) + 
  theme(plot.background = element_blank(), 
        axis.ticks = element_line(lineend = "square"), 
        axis.ticks.length = unit(0.25, "lines"))
)

yel <- viridis::magma(1, alpha = 1, begin = 0.9, end = 0.9, direction = 1)
purp <-  viridis::magma(1, alpha = 1, begin = 0.5, end = 0.5, direction = 1)
```




# Introduction

<!-- insert hook -->

"Causal empiricism" [@samii:2016:causal-empiricism] and Bayesian analysis are two growing schools of methodological work in applied political science. Despite the value of each, studies that employ both methodological approaches are exceedingly rare. This is despite the fact that Bayesian analysis and causal inference are regular companions in other fields such as psychology, epidemiology, and biostatistics.
  \todo{field cites}
  <!-- [x] -->
It is also despite the fact that political scientists in both schools regularly describe the benefit of their school using language and examples that borrow from the other. Introductory discussions of Bayesian inference often use experiments to demonstrate the intuition of Bayesian updating, incorporating prior information from previous studies to improve the precision of estimates obtained from new data.^[
  A common example case in Bayesian pedagogy is an analysis of parallel randomized experiments across eight schools by @rubin:1981:eight-schools [see also @gelman:2013:BDA].
]
Advocates of causal inference in turn use Bayesian language to argue that causal identification provide stronger information for researchers to update their priors about causal effects, since the researcher has a strong prior that an experimental study contains zero or minimal confounding bias.^[
  The "illusion of learning from observational research" [@gerber-et-al:2014:obs-learning] is an argument that comes from a formal Bayesian model, and the namesake result is only obtained through the particular choice of priors in the model.
]

<!-- introduce argument -->

Why, then, is "Bayesian causal inference" so rare in political science?
  \todo{Jstor} 
<!-- Keele et al do a Jstor search in RI paper -->
This paper argues that this division in methodological approaches is needless. Bayesian analysis and causal inference are fundamentally compatible and mutually enhancing. I provide both a positive argument for this perspective and a negative argument against the perceived tensions between these two schools. Although each school uses different tactics, both are united in the goal of getting the best parameter estimates possible. Additionally, the advantages of one school do not necessarily conflict with the advantages of the other. Tensions arise from the informal norms surrounding the applied work in each school rather than the formal structure of the underlying methods. The division between schools is artificial in the least, if not outright counter-productive to the progress of causal empiricism in political science.

After laying out the "positive" and "negative" arguments, I put forth practical advice for doing Bayesian causal inference in ordinary empirical contexts, respecting the skeptical norms within causal empiricism while showcasing the theoretical and practical value of Bayesian approaches. Many Bayesian modeling practices are fundamentally conservative---accounting for all sources of parameter uncertainty, regularizing estimates against over-fitting, and down-weighting parameter values that make no sense in the context of the problem---and should appeal to causal empiricists. Furthermore, Bayesian inference has philosophical and practical advantages that should appeal to causal empiricists even when priors do not dramatically affect the treatment effect estimates. I then implement these points of advice in two replicated analyses: a regression discontinuity study of ideologically extreme House candidates [@hall:2015:extremists] and an experiment on public opinion toward unilateral actions by U.S.\ presidents [@reeves-et-al:2017:unilateral].

My goal is not to convince all causal inference practitioners to use Bayesian methods, nor is it to assert the "superiority" of Bayesian methods. Rather, the objective of the paper is to create space for Bayesian methods in political science's push toward rigorous causal approaches. Bayes and causal inference can make each other better even in simple scenarios, but these improvements should be demonstrated, and areas of potential disagreement or misunderstanding should be adjudicated.

<!-- Terminology, familiarity, other resources -->

Throughout the paper I refer to "causal inference" as a category of research designs that allow the estimation of a parameter with a causal interpretation  justified by an identification analysis. This includes randomized experiments as well as model-driven designs such as difference-in-differences, instrumental variables, synthetic control, regression discontinuity, and others. I refer to "Bayesian analysis" as statistical procedures that include models for unobserved parameters (prior distributions) alongside models for observed data (likelihood functions). For the time being I avoid coining any term or initialism for Bayesian causal inference because, as I hope to convey, estimating causal parameters Bayesianly is neither new nor especially remarkable.^[
  Explicit Bayesian treatments of potential outcomes modeling can be found as far back as @rubin:1978:bayesian.
]
  \todo{literal}
<!-- if it isn't remarkable then why am I remarking on it? What's the word I actually want to use? -->
I assume that a reader of this paper has some conceptual familiarity with either causal inference or Bayesian analysis, but not necessarily both. I provide some background information and notation to ground the discussion as it progresses, but readers looking for comprehensive introductions might consult @angrist-pischke:2008:mostly-harmless,  @imbens:2004:nonpar-ATE, @Imbens-Rubin:2015:causal-inf-text, or @Rubin:2008:design-analysis for causal inference, and @gelman:2013:BDA, @jackman:2009:bayesian, or @mcelreath:2015:stats-rethink for Bayesian analysis.
  \todo{audience?}
<!-- a note about who this is for -->




# Shared Goals, Different Tactics

<!-- what it's for, how it works, what it's good for -->

Causal inference and Bayesian analysis can "go together" in political science. This is because both schools are fundamentally interested in the same thing: obtaining the best, most reliable parameter estimates that we can get, given the data. Causal inference and Bayesian analysis employ different tactics to achieve this goal, which I discuss below, but it is valuable to construe their goals this way at the outset.

We might ask whether "obtaining good parameter estimates" is a trivial goal. Don't all statistical methodologies make parameter estimation their primary focus? I think the answer is No. Take null hypothesis significance testing (NHST) as an example. NHST is the prevailing framework for drawing inferences about hypotheses in political science. It operates by measuring the plausibility of the data under an assumed null hypothesis. Point estimates from statistical models are judged to be "statistically significant" if they were sufficiently unlikely to have occurred by chance ($p < \alpha$) if the null hypothesis were true. NHST is a protocol for making inferences about hypotheses, but it does not work by making substantive inferences about what parameters actually are. Instead it assumes that the true parameter is a value that was never actually estimated and (if zero) rarely represents a legitimate competing hypothesis, and then it calculates the probability of the data under this separate hypothesis. The ordinary usage of NHST does not guide a researcher's inference about which non-null parameters are better, since the null is rejected in favor of a nonspecific alternative. Scholarly work on nonparametric NHST underscores this point by describing the researcher's quantity of interest as "the certainty of the causal inference"---the $p$-value---rather than the causal parameter itself [@keele-et-al:2012:RI].^[
  Point estimates and confidence intervals allow narrower inferences about parameters, but this isn't how NHST operates. Furthermore, if confidence intervals are interpreted as ranges of parameter values that are supported by the data, this is already Bayesian inference.
]
It does make sense, then, to highlight that causal inference and Bayesian analysis share a fundamental desire to generate the best parameter estimates because they stand in contrast to the dominant inference paradigm in most empirical social science.^[
  Even if a causal model is interpreted under using NHST, the causal model is its own thing. Furthermore, NHST can be also conducted using Bayesian models, so this discussion of NHST should not be read as a tirade against non-Bayesian analysis.
]

What are the "best" parameter estimates? Statisticians and methodologists formalize this concept using loss functions, decision rules about bias and error (e.g.\ admissibility), and so on.
  \todo{admissible}
<!-- ^[
  Conveniently, it can be shown in from a decision theory perspective that every *admissible* (sometimes-better and never-worse) decision rule is a Bayes rule [@robert:2007:bayesian-decision].
] -->
For applied work, we care about answering our research questions with a sufficient degree of confidence. Causal inference increases our confidence by de-biasing the research design. Bayesian analysis increases our confidence by returning a posterior distribution with value added from prior information. I discuss both of these contributions in turn, and then I briefly discuss a Bayesian potential outcomes model originally described by @rubin:1978:bayesian. 


## Causal Modeling: Better Parameters "by Design" {#sec:tactic-CI}
<!-- Causal inf -->

To achieve the best parameter estimates, causal inference methods develop or implement careful research designs that (i) identify a causal effect as an inherent feature of the design and (ii) specify the variant of the treatment effect as narrowly as the design can justify. These research designs typically are rooted in a model of the "potential outcomes" for units in the data [@rubin:1974:potential-outcomes] and estimated with a statistical model where the variation in the independent variable identifies the effect derived from the potential outcomes model.

Causal empiricism formalizes causal claims with a carefully specified causal model instead of an informal, verbal discussion of control variables and potential confounders. Formal causal models in political science typically use potential outcomes notation, a simple example being the case where a unit $i$ is assigned a treatment states $Z_{i} \in \{0, 1\}$, with $0$ indicating control and $1$ indicating treatment. The outcome $Y_{i}(z)$ is unit $i$'s potential outcomes given $Z_{i} = z$, with $Y_{i}(z = 1)$ being the potential outcome under treatment and $Y_{i}(z = 0)$ being the potential outcome under control. The unit-level treatment effect $T_{i}$ is the difference between $i$'s potential outcomes: $T_{i} \equiv Y_{i}(1) - Y_{i}(0)$. $T_{i}$ can never be directly measured, however, because $i$ can only be assigned to one treatment level at a time [@holland:1986:causal-inf]. As such, causal models require a set of assumptions to estimate the treatment effect from data.
  \todo{pace?}
<!-- give it the care you gave the Bayes section -->

The research designs derived from causal models identify parameters "by design," meaning that a causal interpretation of the estimated parameter flows necessarily from the research design as long as its assumptions are satisfied. These designs are particularly attentive to the mechanism that assigns units to a treatment status, since the model of the assignment mechanism determines which causal estimands can be identified from the study design [@rubin:1991:assignment]. Identification strategies ideally seek treatment assignment mechanisms that are uncorrelated with the units' potential outcomes. The canonical method for obtaining unconfounded treatment status is random assignment into treatment. In situations where assignment is not perfectly uncorrelated, treatment uptake is imperfect, or when the causal structure is indirect or multifaceted, researchers have explored the assumptions required to identify local treatment effects, conditional treatment effects, intent-to-treat effects, treatment-on-treated effects, direct and indirect effects, and marginal component effects.
  \todo{cites}
<!-- samii for reference? An ARPS article? -->


<!-- advantage: unconfounded -->

Parameter estimates from causal inference designs are "better" because the researcher has confidence that the estimates truly represent a causal effect. The causal interpretation is justified by an identification analysis [@keele:2015:causal-inf; @matzkin:2007:identification] that explicates the minimal set of assumptions required to form this causal interpretation. This is in contrast to regression models that merely estimate a conditional mean of $Y$ but are routinely discussed with vague causal language that is difficult to justify on a "selection-on-observables" identifying assumption. As a result, causal parameters avoid both internal and external validity problems^[
  "Pseudo-facts" and "pseudo-generality" [@samii:2016:causal-empiricism].
]
that plague observational research including model misspecification and generalizing beyond common support. When the researcher is confident that bias in a study is small, the researcher is able to update their priors about the causal effect more decisively than in an observational setting where bias could be more pervasive [@gerber-et-al:2014:obs-learning].

<!-- 
Other notes

- misspecification: "This is despite widespread recognition that regression is liable to misspecification and other oversights in translating a causal diagram into a regression specification [see e.g. @keele:2015:causal-inf]."
- specificity: "By identifying specific causal estimands in an explicit causal model, however, the language researchers can use to describe their estimates are restricted by the structure of their research design and the data they have observed for each node in its causal graph." (Samii & cites to LATE things)
- realism, validity: 
  - "Causal modeling and identification analysis are difficult and effortful additions into the research process, but the crucial payoff is that estimates more closely reflect the causal parameters at interest in the researcher's theoretical model of the world."
  - pseudo-facts (internal validity) and pseudo-generality (lack of common support)
  - "a single, paper- length empirical analysis is likely to yield nonspecific causal facts that generalize without strong assumptions"
  - "builds up to general knowledge through incremental accretion of credible findings across a diversity of settings"
  - maybe, against theory tension: "The point is that there is no inherent tension between causal empiricism and theoretical modeling."
- updating: "[@gerber-et-al:2014:obs-learning]"
 -->



<!-- 
What happens when you do causal inference?

- reduced effects?
- noisier data?
- more consistent estimates?

Give this the same talk-up as with Bayes. 

Find some nice examples where the causal model found something that greatly departed from the observational research

-->











## Bayesian Analysis: Better Parameters with Priors and Posteriors {#sec:tactic-bayes}

To achieve the best parameter estimates, Bayesian methods use the posterior distribution. The posterior distribution conveys which parameter values are likely or unlikely after having seen the data. The posterior distribution is unique to Bayesian analysis and has distinctive philosophical and practical benefits. Philosophically, the posterior allows researchers to make direct inferences about which parameters are consistent with the data and with what probability. This can only be done indirectly in non-Bayesian inference; confidence intervals cover the true parameter only in "95% of infinitely repeated samples," whereas Bayesian intervals directly describe the probability that a parameter is within an interval (conditional on the data) even in one-off samples that are never repeated. The posterior distribution reflects uncertainty over all parameters both marginally and conditionally, with no need for post-hoc standard error corrections or approximations that collapse uncertainty for nuisance parameters. The posterior can also be improved with the specification of the prior distribution, which is discussed more below. Practically, Bayesian models can naturally handle multilevel data structures, missing data, and mixture processes, and they make it easy for researchers to calculate and visualize model estimates with posterior samples.

Mechanically, Bayesian models specify a joint probability distribution over all variables in the model, where a "variable" could be observed data (represented as $y$) or unobserved parameters (represented as $\theta$). Specifying a full probability model for data and parameters merely applies the same intuition to parameters as is routinely applied to data: we can't perfectly predict every instantiation of the process that produces data (or parameters), but we use a probability distribution to represent our estimates of that process. This joint distribution begins with a data model that is a function of model parameters, $p(y \mid \theta)$, and a prior distribution over possible parameter values, $p(\theta)$. This joint distribution can be represented as
\begin{align}
  p(y, \theta) &= p(y \mid \theta)p(\theta).
\end{align}
<!-- prior predictive distribution? 
  p(y) = \int􏰧 p(y, \theta) d\theta = 􏰧 p(\theta)p(y\mid \theta) d\theta
-->
The prior distribution $p(\theta)$ inevitably contains parameters that would be unsupported by the data as more data are collected. Researchers learn which parameters are consistent with the data by conditioning the joint distribution on the data using Bayes' Theorem.
\begin{align}
  p(\theta \mid y) &= \frac{p(y \mid \theta)p(\theta)}{p(y)}
\end{align}
Conditioning on the data allows the researcher to "update" the distribution of parameters. This updated distribution, the posterior distribution, represents the parameter values that are most compatible with both the initial model and the data. The exact shape of the posterior is determined by the specificity of the prior and the strength of the signal from the data. When the prior is vague or the data contain a precise signal about likely and unlikely parameters, the posterior more heavily reflects the data, $p(y \mid \theta)$. When the researcher observes enough data, the prior approaches irrelevance. Conversely, when the prior distribution is more precise or the data send a weak signal, the posterior distribution retains more of the prior. Crucially, the intuition of Bayesian updating is agnostic to whether the original model has a causal interpretation; if any model has parameters that can be conditioned on data, then posterior parameter inference is possible.

All of the philosophically appealing intuitions of the posterior hold even when the prior distribution for parameters $p(\theta)$ is vague or flat. Posterior inference can be improved even further by specifying informative priors about the plausible parameter values in the model. Understandably, the issues surrounding priors is where Bayesian analysis is most commonly misunderstood, so I hope to clear up some of these misunderstandings before directly confronting the intersection of Bayes with causal inference.

To represent parameters with probability distributions, Bayesian statistics must regard parameters as "random" variables. This terminology can be confusing, especially from a non-Bayesian perspective where the true parameter is thought to be an unknown but "fixed" value. This idea of the one true parameter works in Bayesian analysis too; the difference is that we are uncertain about the true value, so our uncertainty is represented using a probability distribution. The parameter is not random in the sense that it is fluctuating between ever-changing quantum states. Rather, it is random in the sense that it may be an instantiation of some underlying process that we cannot describe exactly. This is no different from the way we describe data as draws from probability distributions. We can't exactly predict an individual data point, but we have good justification to assume that the accumulation of forces that determine its exact value resemble a probability distribution. Indeed, likelihood functions are exactly like priors for data; the only difference is that "data" is the word we use for a piece of the model that we observe, while "parameter" is a word we use for a piece of the model that is unobserved.^[
  Richard McElreath, "Understanding Bayesian statistics without frequentist language." Talk delivered at Bayes&#64;Lund2017. Accessed via <https://www.youtube.com/watch?v=yakg94HyWdE>
]


Non-Bayesians are often skeptical of using priors to "stack the deck" in favor of a pre-determined conclusion rather than letting the data speak. Luckily, priors are not employed in such a way. Most of the time priors are used to downweight implausible parameters (e.g.\ coefficients or variance terms that exceed the range of the outcome variable) rather than upweight the researcher's preferred hypothesis. Nearly all modeling problems have natural constraints on the possible parameter values. These natural constraints can be incorporated into the model with "weakly informative priors," which I discuss more in Section&nbsp;\@ref(sec:advice) [see also @gelman2006varianceprior]. Flat priors, by contrast, have the primary effect of *upweighting* highly implausible parameter values. Flat priors regard a coefficient of 10 million as equally likely to a coefficient of 1, but a researcher would throw any model away that returned an estimate of 10 million for most outcome scales. By this logic, it is difficult to imagine situations where we can't improve on a flat prior.
  \todo{puffery}
<!-- Intros to Bayes commonly use vocabulary that make it "feel" incompatible with causal inference methods. -->
  \todo{notation}
<!-- mea culpa about two notational regimes
- forewarning that the Bayesian Rubin approach will hew more closely to the Bayesian notation?
?
-->
<!-- Specifying a probability distribution for every variable in the model "merely" applies the same distributional intuition to parameters as is commonly applied to data: we can't predict the exact value of the data (or parameter) given our information about it, but if we could observe it exactly, it more likely comes from an area of its generating distribution that has higher density. [@mcelreath:2015:stats-rethink]. -->
  \todo{elab?}
 <!-- I elaborate on this more below(?). -->
  \todo{Mike B?}
<!-- This makes philosophical sense in the Bayesian framework because we want to learn as much as we can about the variables in our model, but random (or "independent") processes interfere with our ability to obtain perfect knowledge. As such, the distribution represents our best guess about the processes at work in the model. We could describe likelihood functions as "priors" for observed data; we learn which data are more likely by conditioning on parameters. Parameters have a similar intuition, where we condition on data to learn which parameters are more likely. -->

<!-- Imagine you didn't know the value of $X$ before you observe it, but you know that it comes from a N(0,1) process. Given your incomplete information about $X$, an appropriate way to study functions of $X$ would be to study the distribution of values that would arise from $f(X)$. -->


<!-- Bayesian modeling 

- Puff self up
- Key benefits: not the prior but the posterior distribution
  - Priors are in service on the posterior
- Define and explain
  - fitting and posterior inference
  - joint distribution, updated
- about priors
  - researcher knows what parameters shouldn't be found, so can guide the model and rule out nonsense
  - we always know more than the model does
  - high dimension problems with small data
  - synthesize knowledge across conditions or studies
- Don't make negative arguments about clash w/ CI
- But fine to clear up things about Bayes unto itself

-->




## Potential Outcomes as Posterior Distributions

Causal inference improves parameter estimation by de-biasing the research design, and Bayesian inference improves parameter estimation by incorporating prior information to obtain a posterior distribution of parameter values. How would we combine the two perspectives? This section discusses a Bayesian potential outcomes model originally laid out by @rubin:1978:bayesian but simplified for this paper, as well as a reduced-form example of an experiment analyzed using Bayesian priors.

A key feature of Bayesian analysis is that unknown quantities in the model are given probability distributions. Functions of unknown variables, in turn, reflect uncertainty about variables that compose it. This is relevant to causal inference because it changes our interpretation of the unobserved potential outcomes. An unobserved potential outcome $\tilde{y}_{i}$ in a Bayesian framework is represented by a distribution of unknown potential outcomes that reflects posterior uncertainty about model parameters. We obtain the posterior distribution of the unobserved outcome by conditioning on the data. Its marginal posterior distribution is its joint distribution with the updated model parameters, integrating over the parameter values.
\begin{align}
  p(\tilde{y}_{i} \mid y) &= \int p(\tilde{y}_{i}, \theta \mid y) d\theta
\end{align}
This means that we would construe the unit-level treatment effect $T_{i} = y_{i} - \tilde{y}_{i}$ as an expectation that averages over the posterior uncertainty in the model parameters.
  \todo{int}
  <!-- is this integral gibberish? -->
\begin{align}
  p\left(T_{i} \mid y \right) &= \int p\left(y_{i} - \tilde{y}_{i} \mid y\right)d\theta
\end{align}
In English, the posterior distribution of the treatment effect is the distribution of differences between the observed and (posterior) unobserved potential outcomes, marginalizing over the model parameters.

Why is the Bayesian potential outcomes setup appealing? Because parameters are given probability distributions, our epistemic uncertainty about the causal effect is an explicit feature of the potential outcomes model rather than a byproduct of estimation under sampling error. By treating unobserved potential outcomes as conditional on the observed data, the Bayesian model effectively is a missing data model for potential outcomes [@rubin:1978:bayesian]. Unobserved potential outcomes can be simulated directly from the posterior distribution and evaluated during posterior predictive checking [an important part of the Bayesian workflow, see @gabry:2019:visualization]. This provides natural scaffolding to extend causal models with more complex design features such as missing data, non-compliance, and more [e.g.\ @horiuchi2007designing].

<!-- [which could be sampling-based or "design-based", see @abadie-et-al:2017-design-uncertainty] -->

Zooming out from the potential outcomes model, we can see what an experiment would look like if it were analyzed from a Bayesian perspective. Consider an example where individuals are randomly assigned to values of $z \in \{0, 1\}$. The difference of means is commonly estimated using a regression form...
\begin{align}
  y_{i} &= \alpha + \beta\mathit{z}_{i} + \varepsilon_{i},
\end{align}
where $\alpha$ is the control group mean, $\beta$ is the difference in the means of each group,
<!-- ^[
  Sometimes the difference in the means of the treatment and control group is not exactly the same as the mean of the unit-level treatment effects. This can depend on the link function but also whether a researcher wishes to distinguish sampling-based or "design-based" uncertainty [@abadie-et-al:2017-design-uncertainty].
] -->
and $\varepsilon_{i}$ is response-level error. For Bayesian estimation, we would specify the full probability model for all observed data and unobserved parameters. First, the response data are given a probability model implied by the parametric assumption of $\varepsilon_{i}$. (This is not Bayesian.)
\begin{align}
  y_{i} &\sim \mathrm{Normal}\left(\alpha + \beta\mathit{z}_{i}, \sigma_{z[i]} \right)
\end{align}
This example assumes unequal variance across levels of $z$. We would then include priors for the model parameters, which I will black-box with generic $p(\cdot)$ statements..
\begin{align}
  \alpha &\sim p(\alpha) &
  \beta &\sim p(\beta) &
  \sigma_{z} &\sim p(\sigma_{z})
\end{align}
A deeper discussion about priors can be found in Section&nbsp;\@ref(sec:advice). To fit the model, we condition the parameters on the data and achieve posterior distributions for $\alpha$, $\beta$, and $\sigma$. If the priors are flat, then the posterior distribution for the parameters will be proportional to the likelihood. Otherwise, the posterior distribution will retain some contribution from the prior. All the parameters have the same meaning as in a non-Bayesian model; $\beta$ still represents the causal parameter, but we have estimated it using a prior. It is ultimately a modest intervention on the way experiments are ordinarily interpreted.

Digging a little deeper, one practical consideration for Bayesian estimation is the parametric form of the regression, since parameterization affects the intuition of the priors.^[
  This is also not Bayesian, since likelihood-based models have implicitly flat priors that have different implications on different transformed scales. Flat priors on the log-odds scale will behave differently from flat priors in a linear probability model even if the data are the same. Non-Bayesian models don't avoid the issue of priors; they only ignore it!
]
By estimating a treatment effect with a dummy variable, it is very difficult to place a prior on a difference in means that leads to equal priors for both group means. For this reason, it is more straightforward to write the conditional mean of $y_{i}$ explicitly as the mean in each treatment group, $\mu_{z}$:
\begin{align}
  y_{i} &\sim \mathrm{Normal}\left(\mu_{z[i]}, \sigma_{z} \right).
\end{align}
Under this setup, the average treatment effect is $\beta = \mu_{z = 1} - \mu_{z = 0}$. From that starting point, it is straightforward to place equal priors on both treatment groups.




## Bayesian Opportunities {#sec:opportunities}

There are plenty more things a researcher could consider when estimating a causal model with a Bayesian approach. These are circumstances when we have knowledge about how a model should behave that the model is not guaranteed to learn from the data.

- Bounds: We often deal with parameters that have natural bounds. Variance parameters will be positive. Probabilities will not exceed $0$ or $1$.
- Scale: If we know the scale of an outcome variable, we know a lot about what parameters we could expect from the model that generates that data. Standardizing variables in a model can make it easier to set expectations for a model even before outcome data are collected. 
- Multilevel data: Lots of datasets have hierarchical structure. In experimental and causal inference contexts this could be included by clustered randomization, repeated observations within-unit, correlated error across time periods or within clusters of data, and more. Bayesian models allow the researcher to directly estimate the variance at each hierarchical level, sidestepping the need for post-hoc standard error corrections.
- Regularization: Some experiments use several treatments and either compute marginal treatment effects or complex interactions. As the number of interesting comparisons multiply, priors can be used to do partial pooling, regularize estimates against noisy comparisons, and counter the "multiple comparisons" problem.

Political scientists have put Bayesian analysis to work in plenty of observational work for measurement [@clinton2004statistical; @kernell2009giving; @park2004bayesian], text data [@grimmer:2010:bayes-topic-model], heterogeneous effects [@western:1998:causal-heterogeneity], meta-analysis and model averaging [@montgomery-nyhan:2010-bma], time series [@brandt-freeman:2006:bayes-ts], and time-series cross-sectional data [@shor-et-al:2007:bayes-tscs]. It is far less common in experimental and observational causal inference work, but not truly absent. Bayesian methods have been employed to tackle heterogeneous treatments [@green-kern:2012:bart; see also @hill:2011:bart], modeling noncompliance and nonresponse [@horiuchi2007designing], multi-stage models such as instrumental variables [@hollenbach-et-al:2018:bayes-iv], and conjoint experiments [@jensen-et-al:2019:city-polarization]. Other researchers outside of political science have explored Bayesian regression discontinuity outside of political science [e.g.\ @chib-jacobi:2016:bayes-rdd; @branson-at-al:2019:bayes-rdd]. It is notable that Bayesian analysis in causal inference seems relegated to extraordinary modeling problems rather than day-to-day analysis tasks. 

This paper uses two examples from published political science research to demonstrate how these "Bayesian opportunities" arise and how a researcher might safely address them. I will preview these cases now and implement Bayesian approaches later in Section&nbsp;\@ref(sec:demo). These cases were selected because they are helpful examples, not because the initial studies are in need of revision. The original conclusions are not undermined by a Bayesian analysis, but I think they are improved and clarified in their own ways.

```{r hall-data}
hall_flat <- 
  readRDS(here("data", "estimates", "hall", "MC_linear-win-flat.RDS"))
```

```{r hall-tidy}
win_prob_reduction <- tidy(hall_flat, conf.int = TRUE) %>%
  filter(term == "treatment_effect") %>%
  pull(estimate) %>%
  round(2) %>%
  print()
```

```{r invalid-hall}
# What proportion of samples give us invalid intercepts or treatment effects
invalid_hall <- hall_flat %>%
  spread_draws(alpha[treat], treatment_effect) %>%
  spread(key = treat, value = alpha) %>%
  rename(alpha_1 = `1`, alpha_2 = `2`) %>%
  mutate(invalid_a1 = between(alpha_1, 0, 1) == FALSE,
         invalid_a2 = between(alpha_2, 0, 1) == FALSE,
         invalid_treatment = between(treatment_effect, -1, 1) == FALSE,
         invalid_any = invalid_a1 | invalid_a2 | invalid_treatment) %>%
  # pull(invalid) %>%
  # mean() %>%
  print()

p_invalid <- invalid_hall %>%
  pull(invalid_any) %>%
  mean() %>%
  print()


p_invalid_a1 <- invalid_hall %>%
  pull(invalid_a1) %>%
  mean() %>%
  print()


p_invalid_a2 <- invalid_hall %>%
  pull(invalid_a2) %>%
  mean() %>%
  print()

p_invalid_trt <- invalid_hall %>%
  pull(invalid_treatment) %>%
  mean() %>%
  print()
```

```{r plot-hall-flat-params, include = FALSE, fig.show = 'hide'}
# drawing cut lines
cuts <- 
  tibble(`Control Intercept` = c(0, 1), 
         `Treatment Intercept` = c(0, 1), 
         # `Treatment Effect` = c(-1, 1)
         ) %>%
  gather(key = param_name, value = value) %>%
  mutate(param_name = as.factor(param_name))


# pivot-long intercepts
hall_invalid_ints <- invalid_hall %>%
  gather(key = param, value = value, alpha_1, alpha_2) %>%
  mutate(
    invalid = 
      case_when(
        param == "treatment_effect" ~ between(value, -1, 1) == FALSE,
        str_detect(param, "alpha") ~ between(value, 0, 1) == FALSE
      ) %>%
      as.factor(),
    param_name = 
      case_when(param == "alpha_1" ~ "Non-Extremist Intercept\n(Control)",
                param == "alpha_2" ~ "Extremists Intercept\n(Treatment)") %>%
      fct_rev()
  ) %>%
  print()



g_invalid_pars <- 
  ggplot(hall_invalid_ints, aes(x = value)) +
    geom_histogram(binwidth = 0.05,
                   boundary = 1,
                   color = "black", size = 0.25,
                   aes(fill = (invalid)),
                   show.legend = FALSE) +
    facet_wrap(~ param_name, scales = "free_x"
               ) +
    scale_fill_manual(values = c("TRUE" = purp, "FALSE" = yel)) +
    # viridis::scale_fill_viridis(option = "magma", discrete = TRUE,
    #                             begin = 0.9, end = 0.5) +
    scale_x_continuous(breaks = seq(-1, 2, .5)) +
    labs(y = "Count", x = "Probability of Winning General Election") +
    geom_vline(xintercept = c(0, 1), linetype = "dotted") 


# treatment effect stacked hist
g_invalid_trt <- 
  ggplot(invalid_hall, aes(x = treatment_effect)) +
    geom_histogram(binwidth = 0.05,
                   boundary = 1,
                   color = "black", size = 0.25,
                   aes(fill = fct_rev(as.factor(invalid_any))),
                   show.legend = FALSE) +
    facet_wrap(~ "Treatment Effect of Extremism\nat Discontinuity") +
    scale_fill_manual(values = c("TRUE" = purp, "FALSE" = yel)) +
    # viridis::scale_fill_viridis(option = "magma", discrete = TRUE,
    #                             begin = 0.9, end = 0.5,
    #                             direction = -1) +
    labs(y = "Count", x = "Effect on Win Probability") +
    annotate(geom = "text", x = -1, y = 150, 
             label = "Impossible\nparameter\ncombinations",
             size = 3,
             family = "Source Sans Pro",
             vjust = -0.2) +
    annotate(geom = "segment", x = -1, y = 150, xend = -0.75, yend = 100)

g_invalid_pars / g_invalid_trt
```

First is a regression discontinuity study examining the effect of ideological extremism in U.S.\ House elections @hall:2015:extremists. When a primary election between an extremist and a non-extremist candidate is decided by a "coin-flip" result, the discontinuity provides identifying variation in candidate ideology for the general election. The author finds that when the extremist candidate wins the primary, the probability of winning the general election falls by 53 percentage points. This result is obtained from a linear probability model (LPM), which is often preferred because it is simple and can recover unbiased treatment effects at the discontinuity under the right assumptions [@lee-lemieux:2010:rdd]. In this case, we know that the probability of winning the general election is bounded at 0% and 100%, but the model does not. Researchers often accept this "unrealistic" behavior of LPMs because they are easy to interpret. This is understandable, but if one of the advantages of causal inference is to be realistic [@samii:2016:causal-empiricism], it is fair to say that the treatment effect is important enough that it should not be contaminated by this potential problem in the model. In this particular example---which is only a fraction of the larger study, to be clear---the model happens to place a substantial amount of posterior mass on treatment effects that we would consider impossible.


I demonstrate this behavior of the model in Figure&nbsp;\@ref(fig:plot-hall-flat-params), which shows a Bayesian replication of the original model using flat priors. The left two panels show histograms of posterior samples for the probability of winning at the discontinuity for non-extremists ("control," the first panel) and extremists ("treatment," the second panel). Although the posterior means and modes are within the realistic range of data, a large share of posterior samples for non-extremist candidates exceed 100% probability of winning, and some samples for extremists indicate a probability of winning less than 0% or greater than 100%. The rightmost panel plots the histogram of treatment effects using posterior samples, which is the difference in the intercepts for extremists minus non-extremists. Treatment effects that are composed of at least one "impossible" parameter are indicated with a different color. Of all the treatment effects sampled from the posterior  distribution,  `r percent(p_invalid, accuracy = 1)` are a function of at least one impossible parameter value. I correct this behavior with a Bayesian model in Section&nbsp;\@ref(sec:demo-wips), estimating a treatment effect with 20% lower magnitude but also lower variance.
  \todo{howmuch?}
  <!-- how much lower variance? -->

```{r plot-hall-flat-params, results = 'hide', fig.show = 'asis', include = TRUE, fig.width = 5, fig.height = 6, out.width = "100%", fig.cap = "Histogram of posterior samples from Hall's \\citeyearpar{hall:2015:extremists} regression discontinuity design, recreated using a Bayesian model with flat priors. Dashed lines in left two panels indicate the region of possible parameter values."}
```

The second application in this paper is an experimental study of public opinion on unilateral actions by presidents [@reeves-et-al:2017:unilateral]. The original study uses a series of survey experiments to understand why U.S.\ citizens' approval of unilateral action varies across legal and political contexts. I focus on one experiment that manipulated the specific policy tool or prerogative used by the president to justify unilateral action. The control group was asked to state their agreement with the statement, "Presidents should be able to make new policies without having those policies voted on by Congress." Treatments modified this statement; for example, "Presidents should be able to issue *national security directives* to make new policies without having those policies voted on by Congress" (emphasis mine). The set of policy instruments includes executive agreements, executive orders, national security directives, directing cabinet officials, initiating military operations, issuing proclamations, and issuing memoranda. 


```{r original-reeves, out.width = "65%", include = TRUE, fig.cap = "Treatment Effect Estimates from \\citet[Figure 3, p.~458]{reeves-et-al:2017:unilateral}"}
include_graphics(here("writing", "img", "reeves-trt.png"))
```

The original figure of results (from $t$-tests) is shown in Figure&nbsp;\@ref(fig:original-reeves). Treatment effects were all positive, but because most of the one-off effects were statistically insignificant, the authors initially conclude that information about policy instruments or prerogatives has minimal effects. Their interpretation of the effects takes in an informally Bayesian turn when they note the similarity of the effects overall: 

> \onehalfspacing At the same time, [...] the treatment effects displayed in [the figure] are uniformly positive, which suggests that providing more detailed information about the president’s behavior increases public support for that action. Given low levels of public knowledge about the different tools of unilateral action, it is possible that providing respondents with even more specific information about each of them would generate larger differences in levels of support. (p.\ 458)

Interpreting the results this way is partial pooling, done informally. Because the treatments share many similarities, it makes sense to combine some information across treatments to get a general sense of the distribution of similar treatments. This is reasonable; it would be unlikely to observe seven positive estimates if the null hypothesis is true.^[
  For the curious, if we use a binomial distribution to calculate the probability of observing zero negative estimates out of seven under the null, the probability is approximately `r pbinom(0, 7, 0.5) %>% round(3)`.
]
By building a Bayesian model for these treatment effects, we can do partial pooling formally rather than informally. The results in Section&nbsp;\@ref(sec:demo-pooling) show that allowing the model to combine information across treatments leads to more precise estimates for each one-off treatment effect, and it shows that the overall mean of the treatments is positive





# Tensions and Disagreements

There is some tension between Bayesian practices and causal inference as they are currently performed in political science. While I think much of this is owed to misunderstandings across the divide, open adjudication of the tensions whatever their cause should make any prospect for cooperation easier for both sides. I think this paper will be nothing more than a jumping off point and that much more work can be done to link the applications of Bayes and causal inference in political science. I will start by discussing some hesitations about Bayesian analysis from the viewpoint of causal inference, and then I will offer some words to ease those hesitations. The point is not to invalidate or undermine these hesitations; the hesitations are fair and respectable. The point is to work toward common ground where Bayesian analysis can be done in a way that is respectful of the skeptical view.

Much of the tension comes from the supposed clash between minimalist and maximalist approaches to statistical analysis. Causal inference in minimalist, experiments are simple, and nonparametric estimators rely on fewer assumptions. With all that work to identify a causal effect with as few assumptions as possible, why throw all that parsimony away by adding a bunch of priors? 

The response is that I think the view that Bayesian modeling is maximalist is a result of its niche in the field, not because of anything inherent to what Bayesian analysis is. It happens to be the case that Bayesian modeling in political science leans toward maximalism because the flexibility of the framework has made it a tool-of-choice for particular types of problems, but ultimately, Bayesian analysis is "merely" an approach to parameter estimation. It has the flexibility to be maximalist, but it could be put to simple uses as well. And if Bayesians want to participate in causal inference in political science, they should probably get comfortable scaling back the complexity of their models. One way to help normalize Bayesian analysis for "ordinary" data analysis problems would be for Bayesians to do participate in more "ordinary" data analysis.

Causal inference prefers nonparametric models and unbiased estimators as a default [@keele:2015:causal-inf]. Does this rule out Bayesian approaches, which tend to use parametric models that have some bias? Not necessarily. While there are nonparametric approaches to Bayesian analysis [@ghosal-vandervaart:2017:NPB; @muller-et-al:2015:BNP], they tend to be complex and not in the same spirit as nonparametric causal inference. It makes much more sense to model the nonparametric estimate itself as a draw from a the true parameter, which has a prior. I use this approach below to analyze the presidential powers experiment (Section&nbsp;\@ref(sec:demo-pooling)). This can be especially helpful when nonparametric or semiparametric approaches give results that violate reasonable priors, which I also demonstrate with the House elections RDD (Section&nbsp;\@ref(sec:demo-wips)). Moreover it is worth clarifying that while causal identification de-biases the research design, it does not follow that the estimator must be unbiased. Unbiased estimators may be desirable for different reasons, but a biased estimator will not re-introduce confounding into the design.

Before moving away from the subject of complexity, I think it is appropriate to ask whether experiments will always be so simple. As more political scientists adopt causally rigorous research approaches, experiments and other causal methods will probably become more complex. As a result, demand for statistical tools to deal with these problems would likely increase. 

Priors are another area of hesitation. Even if a skeptic of Bayesian methods is convinced that flat priors are unrealistic and upweight extreme estimates, it's fair to question the value of priors in purely a pragrmatic or rhetorical perspective. Suppose that the prior doesn't add anything; if the difference of means is statistically and substantively significant, then what's the point of the prior? On the flip side, suppose that the difference of means is noisy: would you trust a prior that showed you something that the data didn't show? 

This pragmatic view is understandable, but I think it misunderstands where priors are most effective in applied circumstances. Firstly, priors are not tools for dealing with confounding, so the fact some data come from an experiment is not fundamentally at odds with Bayesian analysis. More importantly, priors are most effective by including weak information. It's helpful to think of priors as *stabilizing* parameter estimates, downweighting extremes rather than upweighting the researcher's preferred hypothesis. The primary effect of these priors is to provide just enough structure to regularize models against overfitting to noisy data and thus overrejecting null hypotheses. Would these benefits not be valuable given the growing concern over the replicability of research findings?

There is lastly the issue that if Bayes were given space in causal inference, researchers would have to expend a lot of effort to learn how to interpret Bayesian analysis. This is true, but I think it's a fair trade. Causal inference is also very difficult to learn, and it is likely to drastically alter the trajectory of empirical political science. Learning new things is good. It makes us all smarter, more creative, and more open-minded to diverse methodological approaches. If these methods turn out to be valuable, the fact that there were external pressures to learn them would be a blessing in hindsight.


# Practical Bayes for Skeptical Causal Inference {#sec:advice}

There are opportunities to improve data analysis using Bayesian approaches in ordinary circumstances as well as in extraordinary circumstances. Bayesian analysis is controversial though, so it should be done with caution. This section discusses a handful of Bayesian tricks that can strengthen the estimation of causal parameters without stacking too many difficult assumptions atop the design. This section can serve both as advice to modelers and as reassurances to Bayesian skeptics that the tools of the Bayesian trade can be applied conservatively and carefully.

#### Focus on model-heavy designs and secondary analyses, not the difference in means.
The difference in means is sacrosanct, especially in experimental studies. Always show the difference in means with a flat prior, even if you think it could be improved. Everybody will want to see it without any extra flourishes. Focus Bayesian efforts instead in areas where the researcher relies more on modeling. Some identification strategies are more model-driven than others, such as regression discontinuity and difference-in-differences. Secondary analysis after the main result also tend to be more model-driven as researchers perform robustness checks and test alternative hypotheses. These areas are where priors can noticeably stabilize an estimate and add clear value to the analysis. 

#### Information, not belief.
Priors are sometimes described as a researcher's "beliefs," which is worrisome for empiricism. Most of the time it makes more sense to talk about priors as statements of "information" rather than belief. A prior is an assumption that brings in information that is external to the raw algebra of the model. Although there are subjectivist schools of Bayesian theory, applied work views priors as "just part of the model" and should be "chosen, evaluated, and revised just like all of the other components of the model" [@mcelreath:2015:stats-rethink]. Since any model is an inaccurate depiction of the world, the researcher's actual degree of belief in any prior is zero; priors should instead be understood as part of the *science* of the model because they can be tested by comparing new data against the model's posterior predictions [@gelman-shalizi:2013:bayes-philosophy].


#### Weakly informative priors.
A researcher who wants to make causal inference with Bayesian analysis is walking a fine line. Priors are valuable, but most readers will be skeptical of their usage. Researchers should strike a balance with a *weakly informative prior*, which is designed so that "the information it does provide is intentionally weaker than whatever actual prior knowledge is available" [@gelman2006varianceprior]. The goal is to provide just enough regularizing information to downweight parameters that would be ludicrous or impossible if the researcher were to encounter them, but not so much information to fight against the data.

One helpful approach for developing weakly informative priors is to standardize variables for a model. This makes it much easier to set priors for intercepts, coefficients, and variances by using the standard deviation as a helpful heuristic. We are pretty sure that treatment effects and residual standard errors will not exceed $1$ in most cases. We are even more assured that neither of these parameters should exceed the full range of the outcome data, so other methods of scaling can be used to improve a prior.

```{r, logit-treatment}
n_logit_sims <- 100000

logit_example <- 
  tibble(`Normal(0, 5)` = rnorm(n_logit_sims, 0, 5),
         `Normal(0, 1.5)` = rnorm(n_logit_sims, 0, 1.5),
         `Normal(0, 1)` = rnorm(n_logit_sims, 0, 1),
         `Normal(0, 0.5)` = rnorm(n_logit_sims, 0, 0.5)) %>%
  gather(key = prior, value = sample, contains("Normal")) %>%
  mutate(prob = plogis(sample), 
         effect = prob - 0.5,
         prior = fct_relevel(prior, "Normal(0, 5)", "Normal(0, 1.5)", "Normal(0, 1)")) %>%
  print()

```

```{r plot-logit-treatment, include = TRUE, fig.width = 5, fig.height = 4, out.width = "80%", fig.cap = "Vague and informed priors. Histograms show prior simulations for logistic regression model coefficients. Prior draws are Normal on the log-odds scale but transformed to the probability scale."}
ggplot(data = logit_example, aes(x = prob)) +
  facet_wrap(~ prior, nrow = 2, scales = "free_y") +
  geom_histogram(binwidth = 0.025, boundary = TRUE,
                 fill = yel, 
                 color = "black", size = 0.25) +
  labs(x = "Effect on predicted probability", y = "Prior draws") +
  scale_y_continuous(labels = comma,
                     expand = expand_scale(mult = c(0, .3)))
```

One common example for demonstrating weak information is logistic regression. The log-odds scale extends from $-\infty$ to $+\infty$, but only a small segment of the log-odds scale maps to probabilities that are useful for typical social science. For a control group with a predicted probability of 50%, a treatment effect of 3.0 on the log odds scale is an effect of more than 40 percentage points.^[
  $\mathrm{invlogit}(3) = `r round(plogis(3), 3)`$
]
So for most social science studies, a prior in a logistic regression can safely assume that coefficients of $3$ or larger are pretty unlikely. Figure&nbsp;\@ref(fig:plot-logit-treatment) shows how normal priors on the log odds scale translate into the probability scale. The top-right panel shows that a fairly weak $\mathrm{Normal}\left(0, 5\right)$ prior on the log-odds scale drastically upweighting very large and very small probabilities. A truly flat prior would be even more extreme, but the prior is implicitly flat for a traditional MLE logit model. It isn't until the prior standard deviation is near $1.5$ that the prior looks approximately flat on the probability scale. Priors like $\mathrm{Normal}\left(0, 1\right)$ and $\mathrm{Normal}\left(0, 0.5\right)$ might be too informative for general recommendation, but they are still wider than the effect sizes we often observe in political science.

Weakly informative priors can also be created using principles of maximum entropy. A maximum entropy distribution is the distribution that injects the least amount of prior information given the natural constraints of a problem. For instance, if a process has a mean and a variance, the least informative family of priors for that process is a Normal. These are the same properties that justify the use of likelihood families for generalized linear modeling.
 \todo{cite}
<!-- talk abt entropy -->
At the same time, researchers sometimes deviate from maximum entropy priors and opt for some priors because they have desirable properties [such as flatter tails, @gelman-et-al:2008:weak-logit].

Researcher who include weakly informative priors can always simulate data from the prior. This allows the reader to glance the consequences of certain prior choices and facilitates transparency in model-building.

#### Full uncertainty and regularization with multilevel models.
Since many datasets have hierarchical structure, multilevel models are often appropriate to account for multiple sources of variance and correlated error within groups of data. By doing fully Bayesian estimation for multilevel models, all parameter and hyperparameter uncertainty is reflected in the posterior distribution. This allows researchers to build an honest model about multilevel data while being conservative about the amount of actually-existing variation. 

Multilevel models are also helpful for partial pooling, which allows a model to learn about the shared characteristics of units and shrinks noisy estimates toward the mean of the units. McElreath describes the intuition of partial pooling with a thought experiment about wait times at various cafés.^[
  Mcelreath's example is featured in the second edition of his textbook (forthcoming), but is discussed in online lectures as well (see <https://github.com/rmcelreath/statrethinking_winter2019>).
]
We want to estimate how long it takes to wait in line for a cup of coffee, starting with a vague prior. As we order coffees from various cafés, we gradually learn several things about how long it takes to get a coffee. First, we get a sense how each individual café differs from others. Second, because cafés are pretty similar even if they aren't identical, we learn about the distribution of cafés and use this to pool the information we get from several cafés together. Lastly, because we pooled information about similar cafés, we get a sense of the average and variance of cafés overall, which is what we're usually interested in. Even if we haven't collected much data from a particular café, we have reasonably good expectations about it because our priors have been updated by our data from other cafés. This allows us to regularize estimates against noise, preventing overfitting and type-one errors. We even have reasonable prior estimates for new cafés that we have never visited before because we understand the distribution from which the café is drawn. In short, as we  collect data from multiple clusters (cafés), but we are allowed to combine information across clusters in a way that teaches us about variation both within and between clusters. This is unlike the way we ordinarily handle clusters of data in political science, where we estimate all treatment effects separately with fixed effects. Information thrown away because each cluster is treated as a separate sample. But there are some situations where pooling information across treatments is natural and intuitive, which I demonstrate in Section&nbsp;\@ref(sec:demo-pooling).





# Estimating Causal Models Bayesianly {#sec:demo}


## Regression Discontinuity using Weakly Informative Priors {#sec:demo-wips}

Researchers often encounter modeling scenarios where the problem presents natural constraints on which parameter values are plausible or even possible. I introduced an example with regression discontinuity earlier where flat priors resulted in posterior treatment effect estimates that included "impossible" parameter combinations---predicted win probabilities greater than 100% or less than 0%. This section applies weakly informative priors to overcome this byproduct of the model and return more sensible estimates.

The model we are replicating is the local linear RDD predicting the probability of winning the general election. Hall estimates this binary outcome using a linear probability model, where $y_{dpt}$ equals $1$ if the candidate in district $d$ in party $p$ in election $t$ wins the general election. Treatment is assigned when the extremist primary candidate's margin in the general election ($m_{dpt}$) is greater than 0. I begin by rewriting his model by indexing parameters according to treatment status in order to set priors more easily. Let's index the treatment status with $w \in \{1, 2\}$, which equals 2 if the extremist candidate wins the primary. First the model assumes that $y_{dpt}$ is distributed Normal with a conditional mean that is the prediction from the RD equation,
\begin{align}
  y_{dpt} &\sim \mathrm{Normal}\left( \alpha_{w[dpt]} + \delta_{w[dpt]}M_{dpt}, \sigma \right), 
\end{align}
where $\alpha_{w}$ are the treatment and control intercepts, and $\delta_{w}$ are the coefficients on the extremist primary margin $M_{dpt}$. The treatment effect at the discontinuity is the difference between the intercepts for extremist primary winners and losers: $\alpha_{2} - \alpha_{1}$. At this point, the algebra of the model is no different from the original study.^[
  Hall estimates both conventional and heteroskedasticity-robust standard errors in his paper and reports whichever is largest. For this model, it makes hardly any difference which standard error is used, so for simplicity I make no heteroskedasticity adjustments.
]

For our prior, we do nothing more than truncate the prior distribution for the intercepts at $0$ and $1$. Aside from that, we put no additional prior information about which win probabilities are more likely for extremists or not-extremists. We do this with a uniform prior, $\alpha_{w} \sim \mathrm{Uniform}\left(0, 1\right)$. To isolate the effect of this one change, I will leave the other priors as flat.

```{r read-hall-brm}
# fix
hall_wip <- readRDS(here("data", "estimates", "hall", "brm-win_trunc.RDS"))
hall_flat <- readRDS(here("data", "estimates", "hall", "brm-win_flat.RDS"))
```


```{r}
long_hall_brm <- list("hall_wip" = hall_wip, 
                      "hall_flat" = hall_flat) %>%
  tibble(fit = .,
         spec = names(.)) %>%
  group_by(spec) %>%
  mutate(wide = map(fit, spread_draws, b_control, b_treat)) %>%
  unnest(wide) %>%
  mutate(
    `Non-Extremist\nIntercept (Control)` = ifelse(str_detect(spec, "logit"), plogis(b_control), b_control),
    `Extremists Intercept\n(Treatment)` = ifelse(str_detect(spec, "logit"), plogis(b_treat), b_treat),
    `Treatment Effect of Extremism\nat Discontinuity` = b_treat - b_control)  %>%
  gather(key = param, value = value, -c(spec:b_treat)) 


hall_saved <- long_hall_brm %>%
  group_by(spec, param) %>%
  summarize(mean = mean(value),
            lower = quantile(value, .025),
            upper = quantile(value, .975)) 

```


```{r plot-hall-post, results = 'hide', fig.show = 'asis', include = TRUE, fig.width = 5, fig.height = 6, out.width = "100%", fig.cap = "Comparison on posterior draws from Hall's \\citeyearpar{hall:2015:extremists} RDD with flat vs.\\ weakly informative priors."}
# 
hall_ints <- long_hall_brm %>%
  filter(param != "Treatment Effect of Extremism\nat Discontinuity") %>%
  mutate(param = fct_rev(param)) %>%
  ggplot(aes(x = value)) +
    facet_wrap(~ param, scales = "free") +
    geom_histogram(aes(fill = spec),
                   position = "identity",
                   binwidth = .05, boundary = 1,
                   alpha = 0.8, color = "black", size = 0.25,
                   show.legend = FALSE) +
    scale_fill_manual(values = c("hall_wip" = yel, "hall_flat" = purp)) +
    scale_x_continuous(breaks = seq(-1, 1, .5)) +
    labs(x = "Probability of Winning General Election",
         y = "Count")

hall_trts <- long_hall_brm %>%
  filter(param == "Treatment Effect of Extremism\nat Discontinuity") %>%
  mutate(param = fct_rev(param)) %>%
  ggplot(aes(x = value)) +
    facet_wrap(~ param, scales = "free") +
    geom_histogram(aes(fill = spec),
                   position = "identity",
                   binwidth = .05, boundary = 1,
                   alpha = 0.8, color = "black", size = 0.25) +
    scale_fill_manual(values = c("hall_wip" = yel, "hall_flat" = purp),
                      labels = c("hall_wip" = "Weak Info", "hall_flat" = "Flat")) +
    scale_x_continuous(breaks = seq(-1, 1, .5)) +
    labs(x = "Effect on Win Probability",
         y = "Count",
         fill = NULL) +
    theme(legend.position = c(0.8, 0.8),
          legend.background = element_blank())

hall_ints / hall_trts

```

Figure&nbsp;\@ref(fig:plot-hall-post) compares the posterior distributions for the original flat priors model to this new "weakly informed" model. How did the prior change the posterior inference? The same way that priors normally do: the effect with more prior information is somewhat smaller than the effect with flat priors (by nearly ten points, `r hall_saved %>% filter(spec == "hall_wip") %$% mean[param == "Treatment Effect of Extremism\nat Discontinuity"] %>% round(2)` compared to `r hall_saved %>% filter(spec == "hall_flat") %$% mean[param == "Treatment Effect of Extremism\nat Discontinuity"] %>% round(2)`), but the estimate is also more precise. This is because a good deal of posterior uncertainty was owed to model parameters that were impossible. We removed this problematic behavior from the model by merely specifying that the constants should fall between $0$ and $1$, which we already knew ahead of time. Causal inference is still possible even with this small prior intervention, and our estimate resembles the original finding even though we've improved it.


## Treatment Effects with Regularization and Partial Pooling {#sec:demo-pooling}

Experiments and other identification strategies often provide researchers with clusters of information that can be combined in a principled way. Section&nbsp;\@ref(sec:opportunities) describes one such situation encountered by @reeves-et-al:2017:unilateral in an experimental study that measures how U.S.\ citizen approval of unilateral action by the president varies with information about the prerogative or policy tool employed. The authors estimate the effects of multiple similar treatments using separate $t$-tests but interpret the signal from the $t$-tests jointly (see Figure&nbsp;\@ref(fig:original-reeves) above). This section demonstrates how these treatment effects can be estimated in a way that preserves information about similar units in order to justify the joint interpretation on statistical grounds. This example also demonstrates how researchers can estimate nonparametric treatment effects as a first cut and model the true underlying effects in a Bayesian framework.

Let $\hat{\beta}_{j}$ be the estimated treatment effect for treatment $j \in \{1, \ldots, 7\}$, which is a difference from the control group mean. We assume (using the central limit theorem, as with all conventional means tests) that these estimates are Normal draws from the true mean $\beta_{j}$ and estimated standard error $\hat{\sigma}_{j}$.^[
  In this example we take the value of $\hat{\sigma}_{j}$ to be known, but we could easily give it a prior and make the model even more uncertain. Since the standard deviations in these data are so small, it makes little difference for this demonstration.
]
\begin{align}
  \hat{\beta}_{j} &\sim \mathrm{Normal}\left( \beta_{j}, \hat{\sigma}_{j} \right) %_
\end{align}

Because these treatments are similar but not identical to one another, it makes sense that the original authors wanted to construct a joint interpretation of them, i.e.\ what is the "average" or "overall" effect among these variants of the treatment? This interpretation implicitly assumes that each "true" treatment effect is a result of some underlying process. We build a model of that process using relaxed assumptions that combine information across treatment groups. The idea is that citizens have a *general* attitude toward unilateral power, but that the use of unilateral power in specific contexts are slight deviations from that mean. If we assume that deviations from the overall mean result from an additive accumulation of details about each treatment context, we end up with an assumption that the true effects $\beta_{j}$ are Normally distributed around a common mean $\mu$ with some hierarchical standard deviation $\psi$.
\begin{align}
  \beta_{j} &\sim \mathrm{Normal}\left(\mu, \psi \right)
\end{align}
This approach allows the model to remember what it learned from treatment $1$ when it estimates the effect of treatment $2$, allowing the model to synthesize information across all treatments into its own meta-analysis that results in a posterior estimate of the grand mean of all treatments $\mu$. I also estimate models that assume that $\beta_{j}$ is a draw from a Student's $T$ distribution with $3$ degrees of freedom. This is a more relaxed prior that has fatter tails, giving more prior weight to larger deviations from the center of the distribution.


Naturally we have to specify priors for $\mu$ and $\psi$. I use very weakly informative priors to demonstrate the robustness of the hierarchical model specification. Because the outcome scale is a proportion on a $0$--$1$ scale, the maximum possible range of treatment effects is $(-1, 1)$. As a result, I give $\mu$ a $\mathrm{Uniform}\left(-1, 1\right)$ prior, which does nothing but rule out impossible treatment effects. The prior for the scale parameter $\psi$ is also extremely weak, I use either a $\mathrm{Uniform}\left(0, 2\right)$ or a $\text{Half-Cauchy}\left(0, 1\right)$ prior. Both priors are wider than the possible variance parameters that could reasonably exist; the Half-Cauchy contains the entire range of possible treatment deviations within $\pm 1$ scale distance from the center of the distribution.


```{r read-reeves}
# reeves <- readRDS(here("data", "processed", "reeves-et-al.Rds")) 
reeves <- readRDS(here("data", "processed", "reeves-treatments.Rds"))
```



```{r read-pool-hypermeans}
mus <- readRDS(here("data", "processed", "negative-hypermeans.Rds")) 
```

```{r negative-mu}
mus
```


```{r plot-reeves, include = TRUE, fig.width = 8, fig.height = 4, out.width = "100%", fig.cap = "Treatment effects estimated with partial pooling. Compare to Figure~\\ref{fig:original-reeves}"}
reeves <- reeves %>%
  mutate(spec_label = str_replace_all(specification, "_", "/") %>% 
                      str_replace_all("t/", "T/") %>% 
                      tools::toTitleCase())

ggplot(reeves, aes(x = (treatment), y = estimate)) +
  geom_hline(yintercept = 0) +
  geom_pointrange(
    aes(ymin = conf.low, ymax = conf.high, color = spec_label), 
    position = position_dodge(width = 0.5)
  ) +
  viridis::scale_color_viridis(
    option = "magma", discrete = TRUE, end = 0.75
  ) +
  labs(x = NULL, y = "Treatment Effect",
       color = "Pooling Priors") +
  coord_cartesian(ylim = c(-.1, .2)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.6), 
        # legend.position = c(0.8, 0.3),
        legend.background = element_blank()) +
  scale_y_continuous(labels = percent_format(suffix = " pp.", accuracy = 1))

```

Figure&nbsp;\@ref(fig:plot-reeves) shows the treatment effect estimates from these hierarchical models and the estimated "Overall Effect" ($\mu$) as well. I show the results of four models for each treatment: two different hierarchical priors for the treatments (Normal and $T$) and two different priors for the hierarchical scale parameters (Uniform and Half-Cauchy). Right away, it's clear that the estimates are robust to the choice of priors; what matters is that we pooled the treatments using a model at all. The multilevel model has the effect of shrinking estimates toward the mean of the distribution. This is because the prior for each treatment effect is adapting to the information learned about all treatment effects. This makes sense; if all we know about the true parameter was that it was drawn from a distribution, we would place a stronger bet on the possibility that the true parameter is closer to the mean than it is farther away from it. In this example, the shrinkage estimates supported the authors' original interpretation of the study; if the model is allowed to combine information from similar treatments instead of being forced to forget information, the fact that all initial treatment effects were positive *did* strengthen the case in favor of a positive overall treatment effect. 

The amount of pooling is not an arbitrary decision introduced by the researcher. Rather, the appropriate amount of pooling is learned from the data because the distribution of groups is updated during model fitting. For this reason, the hierarchical prior is commonly called an "adaptive" prior because its parameters ($\mu$ and $\psi$) are estimated from the data rather than specified exactly by the researcher. The researcher can ask the model for more pooling by placing a tighter prior on the hierarchical scale, but this is often unnecessary because pooling can be achieved even with vague and weakly informative priors [@gelman2006varianceprior].^[
  Researchers sometimes object to multilevel or "random effects" models on the grounds that it assumes that groups are *exchangeable*---there is no information that we have that would lead us to assign a different prior for any group or subset of groups. But this is the same assumption that all regression modeling makes at the unit level, and any regression of groups such as U.S.\ states or countries makes the same assumption. The assumption is easier to justify if the hierarchical mean is itself a regression on covariates, in which case exchangeability applies only to the regression errors.
]
At the same time, it is good to remember that the hierarchical prior is just that: a prior. It does not require that the draw of groups in the data has perfectly symmetrical shape. It is more appropriate to imagine that the posterior distribution contains an infinite number of group configurations---some more symmetrical than others---that are weighted by their plausibility given the adaptive prior and the data.

Partial pooling is appropriate for this example because it reflects the way we ordinarily learn about the world, appreciating the variation both within groups and across groups. It allows us to accumulate knowledge over clusters of data rather than throwing away information after each cluster. And because partial pooling leads to regularization, we should think of it as fundamentally *conservative*, protecting against small samples and noisy data. This regularization goes by other names in non-Bayesian contexts: ridge regression, penalized likelihood, and cross-validation approaches to model selection. It is "regression to the mean" as applied to model parameters.

 


# Unfinished Business

The purpose of this paper is not to convert all causal empiricists to Bayesian analysis. Instead it modestly seeks to justify a space for Bayesian analysis in contemporary causal inference work in political science. Research designs with causal identification still present numerous opportunities for Bayesian analysis, but applied researchers have little precedent in the literature for seizing those opportunities. My goal for this paper is to help build that precedent by confronting the issue directly. Looking forward, many unresolved issues still remain. 

Bayesian analysis is not automatic. Building a model with appropriate assumptions takes effort and can be slow. Although there are many small decisions a researcher could make when building a model (a problem that is not unique to Bayesian analysis), the costs of bespoke model-building deters the researcher from building 100 variations of a model and showcasing the nicest one. That said, if Bayesian analysis should proliferate in the world of causal inference, researchers should probably focus on good rules of thumb to keep analysts from taking too many liberties with an analysis.

Pre-analysis plans (PAPs) are important for Bayesian and non-Bayesian analysis alike to manage the issue of "researcher degrees of freedom." For Bayesian causal analysis, PAPs should contain prior predictive simulations to check the reasonableness of model assumptions. In future work I plan to demonstrate how prior predictive simulations can be incorporated into a `DeclareDesign` workflow to reassure audiences that Bayesian approaches are not being used to $p$-hack an analysis (which is hard to do in Bayesian analysis anyway because most priors regularize against large, noisy effects).

In conclusion, the causal inference movement in political science asks us to think harder and think better about how we estimate parameters. Bayesian causal inference helps us think harder about estimation in addition to research design.


